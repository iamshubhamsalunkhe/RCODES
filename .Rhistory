dim(data)
set.seed(1)
data <- train[1:400,]
train <- data[1:400,]
test <- data[-train,]
test <- data [-train,]
test <- data [401:529,]
colnames(train)
str(data)
data$Dependents <- as.factor(data$Dependents)
data$LoanAmount <- as.numeric(data$LoanAmount)
data$Loan_Amount_Term <- as.numeric(data$Loan_Amount_Term)
View(data)
data$Property_Area <- ifelse(data$Property_Area == "Rural" , 0,
ifelse(data$Property_Area == "Urban" , 1 , 2))
View(data)
dim(data)
set.seed(1)
train <- data[1:400,]
test <- data [401:529,]
colnames(train)
train <- -c[,1,7,8]
train <- -c(data$Loan_ID,data$ApplicantIncome , data$CoapplicantIncome)
colnames(train)
train <- data[1:400,]
test <- data [401:529,]
colnames(train)
train$Loan_ID <- NULL
colnames(train)
train$ApplicantIncome <- NULL
train$CoapplicantIncome <- NULL
colnames(train)
library(e1071)
data_model <- naiveBayes(train$Loan_Status ~ ., data = train)
summary(data_model)
data_model
model_pred <- predict(data_model ,test)
table(model_pred , test$Loan_Status)
22+86/22+5+16+86
(22+86)/22+5+16+86
22+5+16+86
(22+86)/129
library(mlr)
task = makeClassifTask(data = test , target = "Loan_Status")
task
selected_model = makeLearner("classif.naiveBayes")
selected_model
#train the model
NB_mlr = train(selected_model , task)
NB_mlr
# The summary of the model which was printed in e3071 package is stored in
# learner model. Let's print it and compare
NB_mlr$learner.model
test$Loan_ID <- NULL
test$ApplicantIncome <- NULL
test$CoapplicantIncome <- NULL
colnames(test)
data_model <- naiveBayes(train$Loan_Status ~ ., data = train)
data_model
model_pred <- predict(data_model ,test)
table(model_pred , test$Loan_Status)
(22+86)/129
library(mlr)
task = makeClassifTask(data = test , target = "Loan_Status")
task
selected_model = makeLearner("classif.naiveBayes")
selected_model
#train the model
NB_mlr = train(selected_model , task)
NB_mlr
NB_mlr$learner.model
colnames(test)
NBmlr_pred <- as.data.frame(predict(NB_mlr , newdata = test[,-10]))
summary(NBmlr_pred)
table(NBmlr_pred ,test$Loan_Status)
table(NBmlr_pred [,1],test$Loan_Status)
22+3+88+15
(22+88)/128
selected_model <- makeLearner("classif.naiveBayes",
predict.threshold = 0.6,
predict.type = 'prob')
selected_model
NB_mlr <- train(selected_model,task)
NB_mlr
NB_mlr$learner.model
NBmlr_pred <- as.data.frame(predict(NB_mlr ,newdata = test[,-10]))
summary(NBmlr_pred)
head(NBmlr_pred)
table(NBmlr_pred[,3],test$Loan_Status)
23+3+15+88
(23+88)/129
table(NBmlr_pred[,1],test$Loan_Status)
table(NBmlr_pred[,3],test$Loan_Status)
23+3+15+88
(23+88)/129
rm(list = ls())
data <- read.csv(file.choose())
head(data)
str(data)
data$Dependents <- as.factor(data$Dependents)
data$LoanAmount <- as.numeric(data$LoanAmount)
data$Loan_Amount_Term <- as.numeric(data$Loan_Amount_Term)
data$Household_income <- data$ApplicantIncome + data$CoapplicantIncome
head(data$Household_income)
colSums(is.na(data))
data <- na.omit(data)
data$Dependents <- ifelse(data$Dependents == 0 , 0,1)
head(data$Dependents)
data$Property_Area <- ifelse(data$Property_Area == "Rural" , 0,
ifelse(data$Property_Area == "Urban" , 1 , 2))
View(data)
dim(data)
set.seed(1)
train <- data[1:400,]
test <- data [401:529,]
colnames(train)
train$Loan_ID <- NULL
train$ApplicantIncome <- NULL
train$CoapplicantIncome <- NULL
test$Loan_ID <- NULL
test$ApplicantIncome <- NULL
test$CoapplicantIncome <- NULL
colnames(train)
colnames(test)
glm.fits <- glm(train$Loan_Status ~ ., data = train , family = binomial)
summary(glm.fits)
coef(glm.fits)
summary(glm.fits)$coef
summary(glm.fits)$coef[,4]
glm.prob <- predict(glm.fits , type = "response")
glm.prob[1:10]
contrasts(Direction)
contrasts(train$Loan_Status)
dim(train)
glm.pred <- rep("NO" , 400)
glm.pred[glm.prob > .6] = yes
glm.pred[glm.prob > .6]="YES"
round(glm.prob[1:10], 2)
glm.pred[1:10]
glm.pred[1:20]
round(glm.prob[1:20], 2)
glm.pred[1:20]
glm.prob <- predict(glm.fits ,newdata = test, type = "response")
glm.prob[1:10]
contrasts(train$Loan_Status)
contrasts(test$Loan_Status)
dim(test)
##this comman creates a vector where all elements are no in glm.pred
glm.pred <- rep("NO" , 129)
# this code gives yes to probabilities of glm.prob greater than 0.6 in glm .pred
glm.pred[glm.prob > .6]="YES"
round(glm.prob[1:20], 2)
glm.pred[1:20]
table(glm.fits , test)
table(glm.fits , glm.pred)
table(glm.pred , test$Loan_Status)
mean(glm.pred==test$Loan_Status)
mode(glm.pred==test$Loan_Status)
#glm.pred  N  Y
#NO  25 12
#YES 13 79
25+12+13+79
(25+79)/129
(25+79)/129
#0.8062016
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
ncol(Boston)
library(randomForest)
set.seed(1)
bag.boston=randomForest(medv~., data=Boston, subset=train,
mtry=13, importance=TRUE)
bag.boston
yhat.bag = predict(bag.boston, newdata=Boston[-train, ])
boston.test=Boston[-train, "medv"]
mean((yhat.bag-boston.test)^2)
bag.boston=randomForest(medv~., data=Boston, subset=train,
mtry=13, ntree=25)
yhat.bag = predict(bag.boston, newdata=Boston[-train, ])
mean((yhat.bag-boston.test)^2)
rf.boston=randomForest(medv~., data=Boston, subset=train,
mtry=6, importance=TRUE)
yhat.rf = predict(rf.boston, newdata=Boston[-train, ])
mean((yhat.rf-boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
set.seed(1)
x=matrix(rnorm(200*2), ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y)
train=sample(200,100)
svmfit=svm(y~., data=dat[train,], kernel="radial",  gamma=1, cost=1)
library(e1071)
svmfit=svm(y~., data=dat[train,], kernel="radial",  gamma=1, cost=1)
plot(svmfit, dat[train,])
summary(svmfit)
svmfit1=svm(y~., data=dat[train,], kernel="radial",gamma=1,cost=1e5)
plot(svmfit1,dat[train,])
set.seed(1)
tune.out=tune(svm, y~., data=dat[train,], kernel="radial", ranges=list(cost=c(0.1,1,10,100,1000),
gamma=c(0.5,1,2,3,4)))
summary(tune.out) #check lowest error
tune.out$best.model
table(true=dat[-train,"y"], pred=predict(tune.out$best.model,newdata=dat[-train,]))
library(e1071)
library(ISLR)
?Khan
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
length(Khan$ytrain)
length(Khan$ytest)
table(Khan$ytrain)
table(Khan$ytest)
rm(list=ls())
data <- read.csv(file.choose())
head(data)
dim(data)
data <- dataasso
datasso <- data
library(arules)
library(arulesViz)
itemFrequencyPlot(datasso , type = "absolute")
datasso <- as.data.frame(datasso)
itemFrequencyPlot(datasso , type = "absolute")
itemFrequencyPlot(datasso[,-1] , type = "absolute")
rm(list=ls())
setwd("~/Rcodes/marketbasketanalysis")
lastfm <- read.csv("lastfm.csv")
lastfm[1:19,]
length(lastfm$user) ## 289,955 records in the file
lastfm$user <- factor(lastfm$user)
source('~/Rcodes/marketbasketanalysis/lastfm_playlist_Association.R', echo=TRUE)
levels(lastfm$user) ## 15,000 users
levels(lastfm$artist) ## 1,004 artists
library(arules) ## a-rules package for association rules
playlist <- split(x=lastfm[,"artist"],f=lastfm$user) ## split into a list of users
playlist
playlist <- lapply(playlist,unique) ## remove artist duplicates
playlist[1:2]
playlist <- as(playlist,"transactions")
playlist
itemFrequency(playlist)
itemFrequencyPlot(playlist,support=.08,cex.names=1.5)
### *** Play counts *** ###
lastfm <- read.csv("lastfm.csv")
lastfm[1:19,]
length(lastfm$user) ## 289,955 records in the file
lastfm$user <- factor(lastfm$user)
levels(lastfm$user) ## 15,000 users
levels(lastfm$artist) ## 1,004 artists
playlist <- split(x=lastfm[,"artist"],f=lastfm$user) ## split into a list of users
playlist[1:2]
playlist <- lapply(playlist,unique) ## remove artist duplicates
playlist[1:2]
playlist <- as(playlist,"transactions")
playlist
musicrules <- apriori(playlist,parameter=list(support=.01,confidence=.5))
inspect(musicrules)
inspect(subset(musicrules, subset=lift > 5))
inspect(sort(subset(musicrules, subset=lift > 5), by="confidence"))
rm(list = ls())
set.seed(500)
library(MASS)
data <- Boston
apply(data ,2 , function(x) sum(is.na(x)))
index <- sample(1:nrow(data) , round(0.75  * nrow(data)) )
train <- data[index ,]
test <- data[-index,]
lm.fitv <- glm(medv ~ . , data = train)
summary(lm.fitv)
pr.lm <- predict(lm.fitv , test)
mse <- sum((pr.lm - test$medv ) ^ 2) / nrow(test)
mse
maxs <- apply(data, 2 , max) # 2 indicates columns
maxs
mins <- apply(data , 2 , min)
mins
head(scale(data , center = mins , scale = maxs - mins ))
scaled <- as.data.frame(scale(data,center = mins , scale = maxs - mins))
train <- scaled[index,]
test <- scaled[-index,]
install.packages("neuralnet")
library(neuralnet)
n <- names(train)
n
paste(n[n != "medv"] , collapse = " + ")
paste("medv ~" , paste(n[n != "medv"] , collapse = " + "))
as.formula(paste("medv ~" , paste(n[!n "medv"] , collapse = " + ")))
as.formula(paste("medv ~" , paste(n[!n %in%"medv"] , collapse = " + ")))
f <- as.formula(paste("medv ~" , paste(n[n != "medv"],collapse = " + ")))
f
nn <- neuralnet(f , data = train , hidden = c(5 , 3) , linear.output = T)
pr.nn <- compute(nn , test [,1:13])
pr.nn <- pr.nn$net.result * (max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
mse.nn <- sum((test.r - pr.nn)^2) /nrow(test)
mse.nn
print(paste(mse ,mse.nn))
par(mfrow = c(1,2))
plot(test$medv , pr.nn , col ="red" , main= "Real vs Predicted NN" ,
pch = 18 , cex = 0.7)
abline(0,1 , lwd = 2)
legend('bottomright' , legend = 'NN' , pch = 18 , col = 'red' , bty = 'n' )
plot(test$medv , pr.lm , col = 'blue' , main = 'Real vs predicted lm' ,
pch = 18  cex = 0.7)
plot(test$medv , pr.lm , col = 'blue' , main = 'Real vs predicted lm' ,
pch = 18  cex = 0.7)
plot(test$medv , pr.lm , col = 'blue' , main = 'Real vs predicted lm' ,
pch = 18 , cex = 0.7)
abline(0,1,lwd = 2)
legend('bottomright' , legend = 'LM' , pch = 18 , col = 'blue' ,
bty = 'n', cex = .95)
abline(0,1,lwd = 2)
abline(0,1 , lwd = 2)
abline(0,1,lwd = 2)
par(mfrow = c(1,2))
plot(test$medv , pr.nn , col ="red" , main= "Real vs Predicted NN" ,
pch = 18 , cex = 0.7)
abline(0,1 , lwd = 2)
legend('bottomright' , legend = 'NN' , pch = 18 , col = 'red' , bty = 'n' )
plot(test$medv , pr.lm , col = 'blue' , main = 'Real vs predicted lm' ,
pch = 18 , cex = 0.7)
abline(0,1,lwd = 2)
legend('bottomright' , legend = 'LM' , pch = 18 , col = 'blue' ,
bty = 'n', cex = .95)
plot(test$medv , pr.nn , col ="red" , main= "Real vs Predicted NN" ,
pch = 18 , cex = 0.7)
abline(0,1 , lwd = 2)
legend('bottomright' , legend = 'NN' , pch = 18 , col = 'red' , bty = 'n' )
plot(test$medv , pr.lm , col = 'blue' , main = 'Real vs predicted lm' ,
pch = 18 , cex = 0.7)
abline(0,1,lwd = 2)
legend('bottomright' , legend = 'LM' , pch = 18 , col = 'blue' ,
bty = 'n', cex = .95)
abline(0,1,lwd=2)
plot(test$medv , pr.nn , col ="red" , main= "Real vs Predicted NN" ,
pch = 18 , cex = 0.7)
abline(0,1,lwd=2)
rm(list = ls())
set.seed(500)
library(MASS)
data <- Boston
##check that no datapoint is missing
apply(data ,2 , function(x) sum(is.na(x)))
index <- sample(1:nrow(data) , round(0.75  * nrow(data)) )
train <- data[index ,]
test <- data[-index,]
lm.fitv <- glm(medv ~ . , data = train)
summary(lm.fitv)
pr.lm <- predict(lm.fitv , test)
mse <- sum((pr.lm - test$medv ) ^ 2) / nrow(test)
mse
##########
maxs <- apply(data, 2 , max) # 2 indicates columns
maxs
mins <- apply(data , 2 , min)
mins
head(scale(data , center = mins , scale = maxs - mins ))
scaled <- as.data.frame(scale(data,center = mins , scale = maxs - mins))
train <- scaled[index,]
test <- scaled[-index,]
library(neuralnet)
n <- names(train)  #isolating names
n
paste(n[n != "medv"] , collapse = " + ")
paste("medv ~" , paste(n[n != "medv"] , collapse = " + "))
as.formula(paste("medv ~" , paste(n[!n %in%"medv"] , collapse = " + ")))
f <- as.formula(paste("medv ~" , paste(n[n != "medv"],collapse = " + ")))
f
nn <- neuralnet(f , data = train , hidden = c(5 , 3) , linear.output = T)
pr.nn <- compute(nn , test [,1:13])
pr.nn <- pr.nn$net.result * (max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
mse.nn <- sum((test.r - pr.nn)^2) /nrow(test)
mse.nn
##compare the two mses
print(paste(mse ,mse.nn))
plot(test$medv , pr.nn , col ="red" , main= "Real vs Predicted NN" ,
pch = 18 , cex = 0.7)
plot(test$medv , pr.nn , col ="red" , main= "Real vs Predicted NN" ,
pch = 18 , cex = 0.7)
abline(0,1,lwd=2)
legend('bottomright' , legend = 'NN' , pch = 18 , col = 'red' , bty = 'n' )
par(mfrow = c(1,2))
plot(test$medv , pr.nn , col ="red" , main= "Real vs Predicted NN" ,
pch = 18 , cex = 0.7)
abline(0,1,lwd=2)
legend('bottomright' , legend = 'NN' , pch = 18 , col = 'red' , bty = 'n' )
plot(test$medv , pr.lm , col = 'blue' , main = 'Real vs predicted lm' ,
pch = 18 , cex = 0.7)
abline(0,1,lwd = 2)
legend('bottomright' , legend = 'LM' , pch = 18 , col = 'blue' ,
bty = 'n', cex = .95)
train_<- scaled[index,]
test_ <- scaled[-index,]
library(neuralnet)
n <- names(train)  #isolating names
n
paste(n[n != "medv"] , collapse = " + ")
paste("medv ~" , paste(n[n != "medv"] , collapse = " + "))
as.formula(paste("medv ~" , paste(n[!n %in%"medv"] , collapse = " + ")))
f <- as.formula(paste("medv ~" , paste(n[n != "medv"],collapse = " + ")))
f
nn <- neuralnet(f , data = train , hidden = c(5 , 3) , linear.output = T)
pr.nn <- compute(nn , test [,1:13])
pr.nn <- pr.nn$net.result * (max(data$medv)-min(data$medv))+min(data$medv)
train_<- scaled[index,]
test_ <- scaled[-index,]
n <- names(train_)  #isolating names
n
paste(n[n != "medv"] , collapse = " + ")
paste("medv ~" , paste(n[n != "medv"] , collapse = " + "))
as.formula(paste("medv ~" , paste(n[!n %in%"medv"] , collapse = " + ")))
f <- as.formula(paste("medv ~" , paste(n[n != "medv"],collapse = " + ")))
f
nn <- neuralnet(f , data = train_ , hidden = c(5 , 3) , linear.output = T)
pr.nn <- compute(nn , test_ [,1:13])
test.r <- (test_$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
mse.nn <- sum((test.r - pr.nn)^2) /nrow(test_)
mse.nn <- sum((test.r - pr.nn)^2) /nrow(test_)
rm(list = ls())
set.seed(500)
library(MASS)
data <- Boston
##check that no datapoint is missing
apply(data ,2 , function(x) sum(is.na(x)))
index <- sample(1:nrow(data) , round(0.75  * nrow(data)) )
train <- data[index ,]
test <- data[-index,]
lm.fitv <- glm(medv ~ . , data = train)
summary(lm.fitv)
pr.lm <- predict(lm.fitv , test)
mse <- sum((pr.lm - test$medv ) ^ 2) / nrow(test)
mse
##########
maxs <- apply(data, 2 , max) # 2 indicates columns
maxs
mins <- apply(data , 2 , min)
mins
head(scale(data , center = mins , scale = maxs - mins ))
scaled <- as.data.frame(scale(data,center = mins , scale = maxs - mins))
train_ <- scaled[index,]
test_ <- scaled[-index,]
library(neuralnet)
n <- names(train_)  #isolating names
n
paste(n[n != "medv"] , collapse = " + ")
paste("medv ~" , paste(n[n != "medv"] , collapse = " + "))
as.formula(paste("medv ~" , paste(n[!n %in%"medv"] , collapse = " + ")))
f <- as.formula(paste("medv ~" , paste(n[n != "medv"],collapse = " + ")))
f
nn <- neuralnet(f , data = train_ , hidden = c(5 , 3) , linear.output = T)
pr.nn <- compute(nn , test_ [,1:13])
pr.nn <- pr.nn$net.result * (max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
mse.nn <- sum((test.r - pr.nn)^2) /nrow(test)
mse.nn
mse.nn <- sum((test.r - pr.nn)^2) /nrow(test_)
mse.nn
test.r <- (test_$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
mse.nn <- sum((test.r - pr.nn)^2) /nrow(test_)
mse.nn
print(paste(mse ,mse.nn))
par(mfrow = c(1,2))
plot(test$medv , pr.nn , col ="red" , main= "Real vs Predicted NN" ,
pch = 18 , cex = 0.7)
abline(0,1 , lwd = 2)
legend('bottomright' , legend = 'NN' , pch = 18 , col = 'red' , bty = 'n' )
plot(test_$medv , pr.lm , col = 'blue' , main = 'Real vs predicted lm' ,
pch = 18 , cex = 0.7)
abline(0,1,lwd = 2)
plot(test_$medv , pr.lm , col = 'blue' , main = 'Real vs predicted lm' ,
pch = 18 , cex = 0.7)
abline(0,1,lwd=2)
par(mfrow = c(1,2))
plot(test$medv , pr.nn , col ="red" , main= "Real vs Predicted NN" ,
pch = 18 , cex = 0.7)
abline(0,1 , lwd = 2)
legend('bottomright' , legend = 'NN' , pch = 18 , col = 'red' , bty = 'n' )
plot(test$medv , pr.lm , col = 'blue' , main = 'Real vs predicted lm' ,
pch = 18 , cex = 0.7)
abline(0,1,lwd=2)
legend('bottomright' , legend = 'LM' , pch = 18 , col = 'blue' ,
bty = 'n' , cex = .95)
library(boot)
set.seed(200)
lm.fit <- glm(medv ~  ., data = data)
cv.glm(data , lm.fit, K = 10)$delta[1]
set.seed(450)
cv.error <- NULL
k <- 10
library(plyr)
pbar <- create_progress_bar('text')
pbar$init(k)
for(i in 1:k){
index <- sample(1:nrow(data), round(0.9*nrow(data)))
train.cv <- scaled[index,]
test.cv <- scaled[-index,]
nn <- neuralnet(f, data = train.cv, hidden = c(5,2), linear.output = T)
pr.nn <- compute(nn,test.cv[,1:13])
pr.nn <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.cv.r <- (test.cv$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
cv.error[i] <- sum((test.cv.r - pr.nn)^2)/nrow(test.cv)
pbar$step()
}
mean(cv.error)
cv.error
